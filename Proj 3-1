#include <stdio.h>
#include <emmintrin.h>

#define KERNX 3 //this is the x-size of the kernel. It will always be odd.
#define KERNY 3 //this is the y-size of the kernel. It will always be odd.
int conv2D(float* in, float* out, int data_size_X, int data_size_Y,
           float* kernel)
{


    
    
    __m128 k0,k1,k2,k3,k4,k5,k6,k7,k8, m0, m1, m2,target, a0,a1,a2;;
    
    //k vecs are vectors of each number in the kernal
    k0 = _mm_set1_epi32(kernal[0]);
    k1 = _mm_set1_epi32(kernal[1]);
    k2 = _mm_set1_epi32(kernal[2]);
    k3 = _mm_set1_epi32(kernal[3]);
    k4 = _mm_set1_epi32(kernal[4]);
    k5 = _mm_set1_epi32(kernal[5]);
    k6 = _mm_set1_epi32(kernal[6]);
    k7 = _mm_set1_epi32(kernal[7]);
    k8 = _mm_set1_epi32(kernal[8]);

    
    
    /* 
     FLIPPED_kernal:
     8 7 6
     5 4 3
     2 1 0
     */
     
     
     
    /* flipped kernal: 8 5 2cases
    
     
    kern 8*/
        for (int y =0; y<PADDED_data_size_Y;y+=1) {
            for (int x=0;x<PADDED_data_size_X;x+=4)) { // ASSUME PADDED IN ARRAY
                m0 = (_mm_mul_ps(k8, _mm_loadu_si128( (__m128i*)(PADDED_in[x])))); //k8 
                target = _mm_loadu_si128((__m128*)PADDED_out[x+1][y+1]);
                a0 = _mm_add_epi32(m0,target); // integrate relevant elements .... return added multiplied vector to pointer vector
                _mm_store_ps128 (( m128i *) PADDED_out[x+1][y+1], add); //add multiplied vector to OUT array (ASSUMING PADDED OUT)
            }
        }
    
    
    /* kern 5*/
    for (int y =1; y<PADDED_data_size_Y;y+=1) {
        for (int x=0;x<PADDED_data_size_X;x+=4)) { // ASSUME PADDED IN ARRAY
            m0 = (_mm_mul_ps(k5, _mm_loadu_si128( (__m128i*)(PADDED_in[x])))); //k5
            target = _mm_loadu_si128((__m128*)PADDED_out[x+1][y+1]);
            a0 = _mm_add_epi32(m0,target); // integrate relevant elements .... return added multiplied vector to pointer vector
            _mm_store_ps128 (( m128i *) PADDED_out[x+1][y+1], add); //add multiplied vector to OUT array (ASSUMING PADDED OUT)
        }
    }
    
    
    /* kern 2*/
    for (int y =2; y<PADDED_data_size_Y;y+=1) {
        for (int x=0;x<PADDED_data_size_X;x+=4)) { // ASSUME PADDED IN ARRAY
            m0 = (_mm_mul_ps(k2, _mm_loadu_si128( (__m128i*)(PADDED_in[x])))); //k2
            target = _mm_loadu_si128((__m128*)PADDED_out[x+1][y+1]);            
            a0 = _mm_add_epi32(m0,target); // integrate relevant elements .... return added multiplied vector to pointer vector
            _mm_store_ps128 (( m128i *) PADDED_out[x+1][y+1], add); //add multiplied vector to OUT array (ASSUMING PADDED OUT)
        }
    }
    
    
    
    
    
    
    
    /* kern 1, 4, 7 cases*/
    
    /*kern1*/
    for (int y =0; y<PADDED_data_size_Y;y+=1) {
        for (int x=0;x<(PADDED_data_size_X;x+=4)) {  
            m0 = (_mm_mul_ps(k1, _mm_loadu_si128( (__m128i*)(PADDED_in[x]))));
            target = _mm_loadu_si128((__m128*)PADDED_out[x][y+1]);  // for elements in the middle, store directly under: [x,y+1]
            add = _mm_add_epi32(multiplied,target); 
            _mm_store_ps128 (( m128i *) PADDED_out[x][y+1], add);  
        }
    }

    /*kern4*/
    for (int y =1; y<PADDED_data_size_Y;y+=1) {
        for (int x=0;x<(PADDED_data_size_X;x+=4)) {
            m0 = (_mm_mul_ps(k4, _mm_loadu_si128( (__m128i*)(PADDED_in[x]))));
            target = _mm_loadu_si128((__m128*)PADDED_out[x][y+1]);  
            add = _mm_add_epi32(multiplied,target);
            _mm_store_ps128 (( m128i *) PADDED_out[x][y+1], add);
        }
    }
    
    /*kern7*/

    for (int y =2; y<PADDED_data_size_Y;y+=1) {
        for (int x=0;x<(PADDED_data_size_X;x+=4)) {
            m0 = (_mm_mul_ps(k7, _mm_loadu_si128( (__m128i*)(PADDED_in[x]))));
            target = _mm_loadu_si128((__m128*)PADDED_out[x][y+1]);
            add = _mm_add_epi32(multiplied,target);
            _mm_store_ps128 (( m128i *) PADDED_out[x][y+1], add);
        }
    }
    
    
    
    
    
    
    
    /* kern 6, 3, 0 cases*/
    
    /*kern6*/
    for (int y =0; y<=PADDED_data_size_Y;y+=1) {
        for (int x=0;x<(PADDED_data_size_X;x+=4)) {
            m0 = (_mm_mul_ps(k6, _mm_loadu_si128( (__m128i*)(PADDED_in[x]))));
            target = _mm_loadu_si128((__m128*)PADDED_out[x][y+1]);  // for elements in the middle, store directly under: [x+,y+1]
            add = _mm_add_epi32(multiplied,target);
            _mm_store_ps128 (( m128i *) PADDED_out[x][y+1], add);
        }
    }
    
    /*kern3*/
    for (int y =1; y<=PADDED_data_size_Y;y+=1) {
        for (int x=0;x<(PADDED_data_size_X;x+=4)) {
            m0 = (_mm_mul_ps(k3, _mm_loadu_si128( (__m128i*)(PADDED_in[x]))));
            target = _mm_loadu_si128((__m128*)PADDED_out[x][y-1]);
            add = _mm_add_epi32(multiplied,target);
            _mm_store_ps128 (( m128i *) PADDED_out[x][y+1], add);
        }
    }
    
    
    /*kern0*/
    for (int y =2; y<=PADDED_data_size_Y;y+=1) {
        for (int x=0;x<(PADDED_data_size_X;x+=4)) {
            m0 = (_mm_mul_ps(k0, _mm_loadu_si128( (__m128i*)(PADDED_in[x]))));
            target = _mm_loadu_si128((__m128*)PADDED_out[x][y-1]);
            add = _mm_add_epi32(multiplied,target);
            _mm_store_ps128 (( m128i *) PADDED_out[x][y+1], add);
        }
    }
    
    
    
/*

VISUAL INFORMATION 
 
 in: 
 
 1 2 3 4 5
 6 7 8 9 1 
 2 3 4 5 6
 
padded in:
 0 0 0 0 0 0 0 0
 0 1 2 3 4 5 0 0
 0 6 7 8 9 1 0 0
 0 2 3 4 5 6 0 0 
 0 0 0 0 0 0 0 0
 
 first vectorization:
 9 9 9 9 | 9 9 9 9
 8 8 8 8 | 8 8 8 8 
 

 
 FLIPPED_kernal:
 8 7 6
 5 4 3
 2 1 0

 
 unflipped_kernal:
 0 1 2
 3 4 5
 6 7 8
 
 
 
 
 store out:f
 0 0 0 0 0 0 0 0 
 0 0 0 0 0 0 0 0 
 0 0 x x x x x x
 
 
 

 1 2 3 4
 2 3 4 5
 
 0 0 0 0 0 0 0 0 
 0 1 2 3 4 0 0 0 
 0 2 3 4 5 0 0 0 
 0 0 0 0 0 0 0 0 
 
 */
 
 
 
 
 
 
    
    
    
    
