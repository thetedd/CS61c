#include <stdio.h>
#include <emmintrin.h>

#define KERNX 3 //this is the x-size of the kernel. It will always be odd.
#define KERNY 3 //this is the y-size of the kernel. It will always be odd.
int conv2D(float* in, float* out, int data_size_X, int data_size_Y,
           float* kernel)
{


    
    int y, x; //initial declaration ... used in for loops
    __m128 k0,k1,k2,k3,k4,k5,k6,k7,k8, m0,target, a0;
    
    //k vecs are vectors of each number in the kernal
    k0 = _mm_set1_ps(kernel[0]);
    k1 = _mm_set1_ps(kernel[1]);
    k2 = _mm_set1_ps(kernel[2]);
    k3 = _mm_set1_ps(kernel[3]);
    k4 = _mm_set1_ps(kernel[4]);
    k5 = _mm_set1_ps(kernel[5]);
    k6 = _mm_set1_ps(kernel[6]);
    k7 = _mm_set1_ps(kernel[7]);
    k8 = _mm_set1_ps(kernel[8]);

    
    
    /* 
     FLIPPED_kernal:
     8 7 6
     5 4 3
     2 1 0
     */
     
     
     
    /* flipped kernal: 8 5 2 cases
     
     
    Update: use _mm_loadu_ps not _mm_loadu_ps! 
 
     
    kern 8*/
        for (y =0; y<data_size_Y;y+=1) { //PADDED_data_size_Y .... changed to data_size_y due to testing 
            for (x=0;x<data_size_X;x+=4) { // ASSUME PADDED IN ARRAY
                m0 = _mm_mul_ps(k8, _mm_loadu_ps(in+x)); //k8
                target = _mm_loadu_ps((out+ (x+1) +(y+1)*data_size_X)); //out[x+1][y+1]);
                a0 = _mm_add_ps(m0,target); // integrate relevant elements .... return added multiplied vector to pointer vector
               _mm_storeu_ps((out + (x+1)+ (y+1)*data_size_X), a0);  //PADDED_out[x+1][y+1] ... add multiplied vector to OUT array (ASSUMING PADDED OUT)
            }
        }


    /* kern 5*/
    for (y =1; y<data_size_Y;y+=1) {
        for (x=0;x<data_size_X;x+=4) { // ASSUME PADDED IN ARRAY
            m0 = _mm_mul_ps(k5, _mm_loadu_ps(in+x)); //k5
            target = _mm_loadu_ps((out+ (x+1) +(y+1)*data_size_X));
            a0 = _mm_add_ps(m0,target); // integrate relevant elements .... return added multiplied vector to pointer vector
            _mm_storeu_ps((out + (x+1)+ (y+1)*data_size_X), a0);  //PADDED_out[x+1][y+1] ... add multiplied vector to OUT array (ASSUMING PADDED OUT)
        }
    }
    
    
    /* kern 2*/
    for (y =2; y<data_size_Y;y+=1) {
        for (x=0;x<data_size_X;x+=4) { // ASSUME PADDED IN ARRAY
            m0 = _mm_mul_ps(k2, _mm_loadu_ps(in+x));
            target = _mm_loadu_ps((out+ (x+1) +(y+1)*data_size_X));
            a0 = _mm_add_ps(m0,target); // integrate relevant elements .... return added multiplied vector to pointer vector
            _mm_storeu_ps((out + (x+1)+ (y+1)*data_size_X), a0);  //PADDED_out[x+1][y+1] ... add multiplied vector to OUT array (ASSUMING PADDED OUT)
        }
    }


    
    
    
    
    /* flipped_kern 7, 4, 1 cases*/
    
    /*kern7*/
    for (y=0; y<data_size_Y;y+=1) {
        for (x=0;x<data_size_X;x+=4) {
            m0 = _mm_mul_ps(k7, _mm_loadu_ps(in+x));
            target = _mm_loadu_ps(out+x + (y+1)*data_size_X);  // for elements in the middle, store directly under: [x,y+1]
            a0 = _mm_add_ps(m0,target);
            _mm_storeu_ps((out + x+ (y+1)*data_size_X), a0); // out[x][y+1]
        }
    }
    
    /*kern4*/
    for (y =1; y<data_size_Y;y+=1) {
        for (x=0;x<data_size_X;x+=4) {
            m0 = _mm_mul_ps(k4, _mm_loadu_ps(in+x));
            target = _mm_loadu_ps(out+x + (y+1)*data_size_X); 
            a0 = _mm_add_ps(m0,target);
            _mm_storeu_ps((out + x+ (y+1)*data_size_X), a0); // out[x][y+1]
        }
    }
    
    /*kern1*/
    
    for (y =2; y<data_size_Y;y+=1) {
        for (x=0;x<data_size_X;x+=4) {
            m0 = _mm_mul_ps(k1, _mm_loadu_ps(in+x));
            target = _mm_loadu_ps(out+x + (y+1)*data_size_X); 
            a0 = _mm_add_ps(m0,target);
            _mm_storeu_ps((out + x+ (y+1)*data_size_X), a0); // out[x][y+1]
        }
    }
    
    
    
    
    
    
    
    
    
    /* flipped_kern 6, 3, 0 cases*/
    
    /*kern6*/
    for (y =0; y<data_size_Y;y+=1) {
        for (x=0;x<data_size_X;x+=4) {
            m0 = _mm_mul_ps(k6, _mm_loadu_ps(in+x));
            target = _mm_loadu_ps(out+x + (y-1)*data_size_X);
            a0 = _mm_add_ps(m0,target);
           _mm_storeu_ps((out + x+ (y-1)*data_size_X), a0);
        }
    }
    
    /*kern3*/
    for (y =1; y<data_size_Y;y+=1) {
        for (x=0;x<data_size_X;x+=4) {
            m0 = _mm_mul_ps(k3, _mm_loadu_ps(in+x));
            target = _mm_loadu_ps(out+x + (y-1)*data_size_X);
            a0 = _mm_add_ps(m0,target);
            _mm_storeu_ps((out + x+ (y-1)*data_size_X), a0);
        }
    }
    
    
    /*kern0*/
    for (y =2; y<data_size_Y;y+=1) {
        for (x=0;x<data_size_X;x+=4) {
            m0 = _mm_mul_ps(k0, _mm_loadu_ps(in+x));
            target = _mm_loadu_ps(out+x + (y-1)*data_size_X);
            a0 = _mm_add_ps(m0,target);
            _mm_storeu_ps((out + x+ (y-1)*data_size_X), a0);
        }
    }
}
    
//void _mm_storeu_si128( __m128i *p, __m128i a )
/*

VISUAL INFORMATION 
 
 in: 
 
 1 2 3 4 5
 6 7 8 9 1 
 2 3 4 5 6
 
padded in:
 0 0 0 0 0 0 0 0
 0 1 2 3 4 5 0 0
 0 6 7 8 9 1 0 0
 0 2 3 4 5 6 0 0 
 0 0 0 0 0 0 0 0
 
 first vectorization:
 9 9 9 9 | 9 9 9 9
 8 8 8 8 | 8 8 8 8 
 

 
 FLIPPED_kernal:
 8 7 6
 5 4 3
 2 1 0

 
 unflipped_kernal:
 0 1 2
 3 4 5
 6 7 8
 
 
 
 
 store out:f
 0 0 0 0 0 0 0 0 
 0 0 0 0 0 0 0 0 
 0 0 x x x x x x
 
 
 

 1 2 3 4
 2 3 4 5
 
 0 0 0 0 0 0 0 0 
 0 1 2 3 4 0 0 0 
 0 2 3 4 5 0 0 0 
 0 0 0 0 0 0 0 0 
 
 */
 
 
 
 
 
 
    
    
    
    
    
    
    
    
    
    
    
    
 
 
 /*  DOESN'T WORK 
    // the x coordinate of the kernel's center
    int kern_cent_X = (KERNX - 1)/2;
    // the y coordinate of the kernel's center
    int kern_cent_Y = (KERNY - 1)/2;
    
    // main convolution loop
    if(x+i>-1 && x+i<data_size_X && y+j>-1 && y+j<data_size_Y){
	for(int x = 0; x < data_size_X; x++){ // the x coordinate of the output location we're focusing on
		for(int y = 0; y < data_size_Y; y++){ // the y coordinate of theoutput location we're focusing on
			for(int i = -kern_cent_X; i <= kern_cent_X; i++){ // kernel unflipped x coordinate
				for(int j = -kern_cent_Y; j <= kern_cent_Y; j++){ // kernel unflipped y coordinate
					// only do the operation if not out of bounds
						//Note that the kernel is flipped
						out[x+y*data_size_X] +=
                        kernel[(kern_cent_X-i)+(kern_cent_Y-j)*KERNX] * in[(x+i) + (y+j)*data_size_X];
					}
				}
			}
		}
	}
	return 1;
}
 
 
 
 //int beginX=0;
 //int beginY=0;
 
 int countkern_row=0; //9, 6, 3, = 0; 8, 5, 2 = 1; 7, 4, 1 = 3
 func get kernal (0)
 start at 0
 get 0, return vectorized 0, 0
 get 1 returned vectorized 1, 1
 get 2, ......
 get 3, return " " , return 0*/
    
    /*switch (countkern_row) {
     case 0: //9 6 3
     for (k0, k2, k5)
     function((_mm_mul_ps(k0, in0)), x, y);
     break;
     case 1: // 8 5 2
     function((_mm_mul_ps())
     break;
     case 2:
     
     default:
     printf("something is wrong");
     break;
     }
     
     
     
     void store_func(_m128 vector, int knum, int knumcount) {
     switch (knum) {
     
     case 0:
     x[knumcount][
     break;
     case 1:
     break;
     case 2:
     
     default:
     printf("something is wrong");
     break;
     
    

*/
